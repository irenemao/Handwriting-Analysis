{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all files in PaHaW dataset\n",
    "files_PaHaW = []\n",
    "fileDir = 'PaHaW_csv'\n",
    "for root, dirs, files in os.walk(fileDir, topdown=False):\n",
    "    for name in dirs:\n",
    "        files_PaHaW.append(os.path.join(root, name))\n",
    "        \n",
    "charts = []\n",
    "\n",
    "for i in files_PaHaW[2:]:\n",
    "    charts.append(os.path.join(i, ''.join([j for j in os.listdir(i) if '__1_1.csv' in j])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all files in directory tree at given path\n",
    "files_HW = getListOfFiles('hw_dataset')\n",
    "\n",
    "files_HW.remove('hw_dataset\\\\SVM_HW.ipynb')\n",
    "files_HW.remove('hw_dataset\\\\readme.txt')\n",
    "# files_HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the whole length of the spiral drawing\n",
    "def calculate_displacement(data):\n",
    "    \n",
    "    temp = np.sum((data['X coordinate'] - data['X coordinate'].shift(1))**2 + (data['Y coordinate'] - data['Y coordinate'].shift(1))**2)\n",
    "    \n",
    "    return np.sqrt(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PaHaW dataset, the last position value is chosen for the same time\n",
    "def calculate_Vel_X(data):\n",
    "\n",
    "    new_df = data.groupby(['time stamp']).nth([-1]).reset_index()\n",
    "    new_df['velocity_x'] = (new_df['X coordinate'] - new_df['X coordinate'].shift(1)) / (new_df['time stamp'] - new_df['time stamp'].shift(1))\n",
    "    \n",
    "    new_df = pd.merge(data, new_df[['time stamp', 'velocity_x']], on='time stamp', how='left')\n",
    "    \n",
    "    return new_df['velocity_x']\n",
    "\n",
    "def calculate_Vel_Y(data):\n",
    "\n",
    "    new_df = data.groupby(['time stamp']).nth([-1]).reset_index()\n",
    "    new_df['velocity_y'] = (new_df['Y coordinate'] - new_df['Y coordinate'].shift(1)) / (new_df['time stamp'] - new_df['time stamp'].shift(1))\n",
    "    \n",
    "    new_df = pd.merge(data, new_df[['time stamp', 'velocity_y']], on='time stamp', how='left')\n",
    "    \n",
    "    return new_df['velocity_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate velocity for ParkingsonHW dataset\n",
    "def cal_Vel_X(data):\n",
    "\n",
    "    velocity_x = (data['X coordinate'] - data['X coordinate'].shift(1)) / (data['time stamp'] - data['time stamp'].shift(1))\n",
    "        \n",
    "    return velocity_x\n",
    "\n",
    "def cal_Vel_Y(data):\n",
    "\n",
    "    velocity_y = (data['Y coordinate'] - data['Y coordinate'].shift(1)) / (data['time stamp'] - data['time stamp'].shift(1))\n",
    "        \n",
    "    return velocity_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate real time velocity for both datasets\n",
    "def calculate_Vel(data):\n",
    "\n",
    "    velocity = np.sqrt(data['vel_x']**2 + data['vel_y']**2)\n",
    "    \n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PaHaW dataset\n",
    "def calculate_Acc_X(data):\n",
    "    \n",
    "    new_df = data.groupby(['time stamp']).nth([-1]).reset_index()\n",
    "    new_df['acc_x'] = (data['vel_x'] - data['vel_x'].shift(1)) / (new_df['time stamp'] - new_df['time stamp'].shift(1))\n",
    "    \n",
    "    new_df = pd.merge(data, new_df[['time stamp', 'acc_x']], on='time stamp', how='left')\n",
    "    \n",
    "    return new_df['acc_x']\n",
    "\n",
    "def calculate_Acc_Y(data):\n",
    "    \n",
    "    new_df = data.groupby(['time stamp']).nth([-1]).reset_index()\n",
    "    new_df['acc_y'] = (data['vel_y'] - data['vel_y'].shift(1)) / (new_df['time stamp'] - new_df['time stamp'].shift(1))\n",
    "    \n",
    "    new_df = pd.merge(data, new_df[['time stamp', 'acc_y']], on='time stamp', how='left')\n",
    "\n",
    "    return new_df['acc_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ParkinsonHW dataset\n",
    "def cal_Acc_X(data):\n",
    "\n",
    "    acc_x = (data['vel_x'] - data['vel_x'].shift(1)) / (data['time stamp'] - data['time stamp'].shift(1))\n",
    "        \n",
    "    return acc_x\n",
    "\n",
    "def cal_Acc_Y(data):\n",
    "\n",
    "    acc_y = (data['vel_y'] - data['vel_y'].shift(1)) / (data['time stamp'] - data['time stamp'].shift(1))\n",
    "        \n",
    "    return acc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Acc(data):\n",
    "    \n",
    "    new_df = data.groupby(['time stamp']).nth([-1]).reset_index()\n",
    "    new_df['acc'] = (data['velocity'] - data['velocity'].shift(1)) / (new_df['time stamp'] - new_df['time stamp'].shift(1))\n",
    "    \n",
    "    new_df = pd.merge(data, new_df[['time stamp', 'acc']], on='time stamp', how='left')\n",
    "\n",
    "    return new_df['acc']\n",
    "\n",
    "def cal_Acc(data):\n",
    "\n",
    "    acc = (data['velocity'] - data['velocity'].shift(1)) / (data['time stamp'] - data['time stamp'].shift(1))\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Normalised Velocity Variance on x_axis velocity, y_axis velocity and velocity\n",
    "def calculate_NVV_X(x):\n",
    "    x = x.replace([np.inf, -np.inf], np.nan)\n",
    "    sum_x = np.nansum(abs((x['vel_x'] - x['vel_x'].shift(1))))\n",
    "    NVV_x = sum_x / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    \n",
    "    return NVV_x\n",
    "    \n",
    "def calculate_NVV_Y(x):\n",
    "    x = x.replace([np.inf, -np.inf], np.nan)\n",
    "    sum_y = np.nansum(abs((x['vel_y'] - x['vel_y'].shift(1))))\n",
    "    NVV_y = sum_y / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    \n",
    "    return NVV_y\n",
    "\n",
    "def calculate_NVV(x):\n",
    "#     x = x.replace([np.inf, -np.inf], np.nan)\n",
    "    sum_y = np.nansum(abs((x['velocity'] - x['velocity'].shift(1))))\n",
    "    NVV = sum_y / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    \n",
    "    return NVV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Normalised Pressure Variance\n",
    "def calculate_NPV_pre(p):\n",
    "    sum_p = np.sum(abs(p['pressure'] - p['pressure'].shift(1)))\n",
    "    NVV_p = sum_p / (p.iloc[-1, 2]-p.iloc[0, 2])\n",
    "    \n",
    "    return NVV_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Jerk_x(x):\n",
    "    \n",
    "    jerk_x = np.sum(abs(x['acc_x'] - x['acc_x'].shift(1))) / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    return jerk_x\n",
    "\n",
    "def calculate_Jerk_y(x):\n",
    "            \n",
    "    jerk_y = np.sum(abs(x['acc_y'] - x['acc_y'].shift(1))) / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    return jerk_y\n",
    "\n",
    "def calculate_Jerk(x):\n",
    "            \n",
    "    jerk = np.sum(abs(x['acc'] - x['acc'].shift(1))) / (x.iloc[-1, 2]-x.iloc[0, 2])\n",
    "    return jerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conventional_energy_x(data):\n",
    "    \n",
    "    x_mean = np.mean(data['X coordinate'])\n",
    "    x_var = np.var(data['X coordinate'], ddof=1)\n",
    "    nor_x = (data['X coordinate']-x_mean)/x_var\n",
    "    \n",
    "    energy_x = np.sum(nor_x ** 2)/len(data['X coordinate'])\n",
    "    \n",
    "    return energy_x\n",
    "\n",
    "def conventional_energy_y(data):\n",
    "    \n",
    "    y_mean = np.mean(data['Y coordinate'])\n",
    "    y_var = np.var(data['Y coordinate'], ddof=1)\n",
    "    nor_y = (data['Y coordinate']-y_mean)/y_var\n",
    "    \n",
    "    energy_y = np.sum(nor_y ** 2)/len(data['Y coordinate'])\n",
    "    \n",
    "    return energy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of changes in velocity normalized on its duration\n",
    "def NCV_max_vel(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    max_vel = df.iloc[argrelextrema(df['velocity'].values, np.greater_equal, order=n)[0]]['velocity']\n",
    "    \n",
    "    max_vel = max_vel.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_max_vel = np.nanmean(max_vel)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_max_vel\n",
    "\n",
    "def NCV_min_vel(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    min_vel = df.iloc[argrelextrema(df['velocity'].values, np.less_equal, order=n)[0]]['velocity']\n",
    "    \n",
    "    min_vel = min_vel.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_min_vel = np.nanmean(min_vel)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_min_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCV on x-axis velocity\n",
    "def NCV_max_x(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    max_vel_x = df.iloc[argrelextrema(df['vel_x'].values, np.greater_equal, order=n)[0]]['vel_x']\n",
    "    \n",
    "    max_vel_x = max_vel_x.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_max_x = np.nanmean(max_vel_x)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_max_x\n",
    "\n",
    "def NCV_min_x(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_vel_x = df.iloc[argrelextrema(df['vel_x'].values, np.less_equal, order=n)[0]]['vel_x']\n",
    "    \n",
    "    min_vel_x = min_vel_x.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_min_x = np.nanmean(min_vel_x)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_min_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCV on y-axis velocity\n",
    "def NCV_max_y(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    max_vel_y = df.iloc[argrelextrema(df['vel_y'].values, np.greater_equal, order=n)[0]]['vel_y']\n",
    "    \n",
    "    max_vel_y = max_vel_y.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_max_y = np.nanmean(max_vel_y)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_max_y\n",
    "\n",
    "def NCV_min_y(df):\n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_vel_y = df.iloc[argrelextrema(df['vel_y'].values, np.less_equal, order=n)[0]]['vel_y']\n",
    "    \n",
    "    min_vel_y = min_vel_y.replace([np.inf, -np.inf], np.nan)\n",
    "    NCV_min_y = np.nanmean(min_vel_y)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCV_min_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of changes in acceleration normalised on its duration\n",
    "def NCA_min_x(df):\n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_acc_x = df.iloc[argrelextrema(df['acc_x'].values, np.less_equal, order=n)[0]]['acc_x']\n",
    "    \n",
    "    min_acc_x = min_acc_x.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_min_x = np.nanmean(min_acc_x)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_min_x\n",
    "\n",
    "def NCA_max_x(df):\n",
    "    \n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    max_acc_x = df.iloc[argrelextrema(df['acc_x'].values, np.greater_equal, order=n)[0]]['acc_x']\n",
    "    \n",
    "    max_acc_x = max_acc_x.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_max_x = np.nanmean(max_acc_x)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_max_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCA_min_y(df):\n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_acc_y = df.iloc[argrelextrema(df['acc_y'].values, np.less_equal, order=n)[0]]['acc_y']\n",
    "    \n",
    "    min_acc_y = min_acc_y.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_min_y = np.nanmean(min_acc_y)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_min_y\n",
    "\n",
    "def NCA_max_y(df):\n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    max_acc_y = df.iloc[argrelextrema(df['acc_y'].values, np.greater_equal, order=n)[0]]['acc_y']\n",
    "    \n",
    "    max_acc_y = max_acc_y.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_max_y = np.nanmean(max_acc_y)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_max_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCA_min(df):\n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_acc = df.iloc[argrelextrema(df['acc'].values, np.less_equal, order=n)[0]]['acc']\n",
    "    \n",
    "    min_acc = min_acc.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_min = np.nanmean(min_acc)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_min\n",
    "\n",
    "def NCA_max(df):  \n",
    "    n=5\n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    max_acc = df.iloc[argrelextrema(df['acc'].values, np.greater_equal, order=n)[0]]['acc']\n",
    "    \n",
    "    max_acc = max_acc.replace([np.inf, -np.inf], np.nan)\n",
    "    NCA_max = np.nanmean(max_acc)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCA_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of changes in pressure\n",
    "def NCP_min(df):\n",
    "    \n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    min_pre = df.iloc[argrelextrema(df['pressure'].values, np.less_equal, order=n)[0]]['pressure']\n",
    "    \n",
    "    min_pre = min_pre.replace([np.inf, -np.inf], np.nan)\n",
    "    NCP_min = np.nanmean(min_pre)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCP_min\n",
    "\n",
    "def NCP_max(df):\n",
    "    \n",
    "    n=5 \n",
    "    # number of points to be checked before and after \n",
    "    # Find local peaks\n",
    "    max_pre = df.iloc[argrelextrema(df['pressure'].values, np.greater_equal, order=n)[0]]['pressure']\n",
    "    \n",
    "    max_pre = max_pre.replace([np.inf, -np.inf], np.nan)\n",
    "    NCP_max = np.nanmean(max_pre)/(df.iloc[-1, 2]-df.iloc[0, 2])\n",
    "\n",
    "    return NCP_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    coor_arr = np.column_stack((data['X coordinate'], data['Y coordinate']))\n",
    "\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth = 0.25).fit(coor_arr)\n",
    "    logprob = kde.score_samples(coor_arr)\n",
    "    \n",
    "    entropy_data = entropy(logprob, base = 2)\n",
    "   \n",
    "    return entropy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def calculate_iqr(data):\n",
    "    \n",
    "    return iqr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data and delete nan or inf values\n",
    "\n",
    "def normalise_data(x):\n",
    "    \n",
    "    x = x.replace([np.inf, -np.inf], np.nan)\n",
    "    x.dropna(axis=0, how='any', inplace=True)\n",
    "    \n",
    "    for column in x:\n",
    "        if column != 'ID' and  column != 'result' and column != 'label':\n",
    "            x_mean = np.mean(x[column])\n",
    "            var_x = np.var(x[column], ddof=1)\n",
    "            x[column] = (x[column]-x_mean)/(var_x)\n",
    "       \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = 2\n",
    "#test_id = 1, dynamic drawing\n",
    "final_HW = []\n",
    "\n",
    "for i in files_HW:\n",
    "    try:\n",
    "        table = pd.read_csv(i, sep=';', index_col=False, header=None, names=['X coordinate', 'Y coordinate', 'Z', 'pressure', 'Grip Angle', 'time stamp', 'Test ID'],\n",
    "                          lineterminator='\\n')\n",
    "        \n",
    "        table = table[table['Test ID'] == 1]\n",
    "        table = table.drop('Z', axis=1)\n",
    "        table = table[['X coordinate', 'Y coordinate', 'time stamp', 'pressure', 'Grip Angle', 'Test ID']]\n",
    "\n",
    "        table['vel_x'] = cal_Vel_X(table)\n",
    "        table['vel_y'] = cal_Vel_Y(table)\n",
    "        table['velocity'] = calculate_Vel(table)\n",
    "        table['acc_x'] = cal_Acc_X(table)\n",
    "        table['acc_y'] = cal_Acc_Y(table)\n",
    "        table['acc'] = cal_Acc(table)\n",
    "        \n",
    "        width = np.amax(table['X coordinate'])-np.amin(table['X coordinate'])\n",
    "        height = np.amax(table['Y coordinate'])-np.amin(table['Y coordinate'])\n",
    "        nvv_x = calculate_NVV_X(table)\n",
    "        nvv_y = calculate_NVV_Y(table)\n",
    "        nvv = calculate_NVV(table)\n",
    "        npv = calculate_NPV_pre(table)\n",
    "        jerk_x = calculate_Jerk_x(table)\n",
    "        jerk_y = calculate_Jerk_y(table)\n",
    "        jerk = calculate_Jerk(table)\n",
    "        nca_max_x = NCA_max_x(table)\n",
    "        nca_min_x = NCA_min_x(table)\n",
    "        nca_max_y = NCA_max_y(table)\n",
    "        nca_min_y = NCA_min_y(table)\n",
    "        nca_max = NCA_max(table)\n",
    "        nca_min = NCA_min(table)\n",
    "        ncv_max_x = NCV_max_x(table)\n",
    "        ncv_max_y = NCV_max_y(table)\n",
    "        ncv_min_x = NCV_min_x(table)\n",
    "        ncv_min_y = NCV_min_y(table)\n",
    "        ncv_max = NCV_max_vel(table)\n",
    "        ncv_min = NCV_min_vel(table)\n",
    "        ncv_nca_max_x = ncv_max_x/nca_max_x\n",
    "        ncv_nca_max_y = ncv_max_y/nca_max_y\n",
    "        ncv_nca_min_x = ncv_min_x/nca_min_x\n",
    "        ncv_nca_min_y = ncv_min_y/nca_min_y\n",
    "        ncv_nca_max = ncv_max/nca_max\n",
    "        ncv_nca_min = ncv_min/nca_min\n",
    "        ncp_max = NCP_max(table)\n",
    "        ncp_min = NCP_min(table)\n",
    "        std_pre = np.std(table['pressure'])\n",
    "        std_grip = np.std(table['Grip Angle'])\n",
    "        mean_grip = np.mean(table['Grip Angle'])\n",
    "        duration = table.iloc[-1, 2]-table.iloc[0, 2]\n",
    "        displacement = calculate_displacement(table)\n",
    "        entropy_S = calculate_entropy(table)\n",
    "        energy_x = conventional_energy_x(table)\n",
    "        energy_y = conventional_energy_y(table)\n",
    "        pressure_iqr = calculate_iqr(table['pressure'])\n",
    "        grip_iqr = calculate_iqr(table['Grip Angle'])\n",
    "        mv = displacement/duration\n",
    "#         sdv = np.std(table['velocity'], ddof=1)\n",
    "        \n",
    "        file_name = os.path.realpath(i).split('\\\\')[-1].split('.')[0]\n",
    "        result = 0\n",
    "        if file_name.split('_')[0] == 'P':\n",
    "            result = 1\n",
    "            ids = int(file_name.split('_')[1])\n",
    "        if file_name.split('_')[0] == 'H':\n",
    "            result = 1\n",
    "            numb = file_name.split('_')[1]\n",
    "            ids = int(numb.split('-')[1])\n",
    "        if file_name.split('_')[0] == 'C':\n",
    "            result = 0\n",
    "            ids = int(file_name.split('_')[1])\n",
    "        label = 2\n",
    "        \n",
    "        final_HW.append([ids, nvv_x, nvv_y, nvv, npv, jerk_x, jerk_y, jerk, nca_max_x, nca_max_y, nca_min_x, nca_min_y, nca_max, nca_min, ncv_max_x, ncv_max_y, ncv_min_x, ncv_min_y, ncv_max, ncv_min, ncv_nca_max_x, ncv_nca_max_y, ncv_nca_min_x, ncv_nca_min_y, ncv_nca_max, ncv_nca_min, ncp_max, ncp_min, std_pre, std_grip, mean_grip, duration, entropy_S, pressure_iqr, grip_iqr, mv, width, height, displacement, energy_x, energy_y, label, result])\n",
    "\n",
    "#         final_HW.append([ids, std_grip, entropy_S, label, result])\n",
    "#         final_HW = normalise_data(final_HW)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# final_HW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#label = 1\n",
    "final = []\n",
    "\n",
    "for i in charts:\n",
    "    try:\n",
    "        df = pd.read_csv(i).reset_index()\n",
    "        if(len(df.columns)==7):\n",
    "            cols = ['Y coordinate', 'X coordinate', 'time stamp', 'on/off state', 'azimuth', 'altitude', 'pressure']\n",
    "            df.columns = cols\n",
    "        \n",
    "        df['vel_x'] = calculate_Vel_X(df)\n",
    "        df['vel_y'] = calculate_Vel_Y(df)\n",
    "        df['velocity'] = calculate_Vel(df)\n",
    "        df['acc_x'] = calculate_Acc_X(df)\n",
    "        df['acc_y'] = calculate_Acc_Y(df)\n",
    "        df['acc'] = calculate_Acc(df)\n",
    "\n",
    "        width = np.amax(df['X coordinate'])-np.amin(df['X coordinate'])\n",
    "        height = np.amax(df['Y coordinate'])-np.amin(df['Y coordinate'])\n",
    "        nvv_x = calculate_NVV_X(df)\n",
    "        nvv_y = calculate_NVV_Y(df)\n",
    "        nvv = calculate_NVV(df)\n",
    "        npv = calculate_NPV_pre(df)\n",
    "        jerk_x = calculate_Jerk_x(df)\n",
    "        jerk_y = calculate_Jerk_y(df)\n",
    "        jerk = calculate_Jerk(df)\n",
    "        nca_max_x = NCA_max_x(df)\n",
    "        nca_min_x = NCA_min_x(df)\n",
    "        nca_max_y = NCA_max_y(df)\n",
    "        nca_min_y = NCA_min_y(df)\n",
    "        nca_max = NCA_max(df)\n",
    "        nca_min = NCA_min(df)\n",
    "        ncv_max_x = NCV_max_x(df)\n",
    "        ncv_max_y = NCV_max_y(df)\n",
    "        ncv_min_x = NCV_min_x(df)\n",
    "        ncv_min_y = NCV_min_y(df)\n",
    "        ncv_max = NCV_max_vel(df)\n",
    "        ncv_min = NCV_min_vel(df)\n",
    "        ncv_nca_max_x = ncv_max_x/nca_max_x\n",
    "        ncv_nca_max_y = ncv_max_y/nca_max_y\n",
    "        ncv_nca_min_x = ncv_min_x/nca_min_x\n",
    "        ncv_nca_min_y = ncv_min_y/nca_min_y\n",
    "        ncv_nca_max = ncv_max/nca_max\n",
    "        ncv_nca_min = ncv_min/nca_min\n",
    "        ncp_max = NCP_max(df)\n",
    "        ncp_min = NCP_min(df)\n",
    "        std_pre = np.std(df['pressure'])\n",
    "        std_grip = np.std(df['altitude'])\n",
    "        mean_grip = np.mean(df['altitude'])\n",
    "        duration = df.iloc[-1, 2]-df.iloc[0, 2]\n",
    "        displacement = calculate_displacement(df)\n",
    "        entropy_S = calculate_entropy(df)\n",
    "        energy_x = conventional_energy_x(df)\n",
    "        energy_y = conventional_energy_y(df)\n",
    "        pressure_iqr = calculate_iqr(df['pressure'])\n",
    "        grip_iqr = calculate_iqr(df['altitude'])\n",
    "        mv = displacement/duration\n",
    "#         sdv = np.std(table['velocity'], ddof=1)\n",
    "        \n",
    "        ids = os.path.dirname(i).split('\\\\')[-1]\n",
    "        label = 1\n",
    "        final.append([ids, nvv_x, nvv_y, nvv, npv, jerk_x, jerk_y, jerk, nca_max_x, nca_max_y, nca_min_x, nca_min_y, nca_max, nca_min, ncv_max_x, ncv_max_y, ncv_min_x, ncv_min_y, ncv_max, ncv_min, ncv_nca_max_x, ncv_nca_max_y, ncv_nca_min_x, ncv_nca_min_y, ncv_nca_max, ncv_nca_min, ncp_max, ncp_min, std_pre, std_grip, mean_grip, duration, entropy_S, pressure_iqr, grip_iqr, mv, width, height, displacement, energy_x, energy_y, label])\n",
    "\n",
    "#         final.append([ids, std_grip, entropy_S, label]) \n",
    "        final.replace('nan', np.nan, inplace=True)\n",
    "        final.isnull().any()\n",
    "        final.fillna(0, inplace=True)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# final\n",
    "### 01-05, 61, 80, 89 missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PaHaW: combine results in the dataframe\n",
    "final = pd.DataFrame(final)\n",
    "final.columns = ['ID', \"nvv_x\", \"nvv_y\", \"nvv\", \"npv\", \"jerk_x\", \"jerk_y\", \"jerk\", \"nca_max_x\", \"nca_max_y\", \"nca_min_x\", \"nca_min_y\", \"nca_max\", \"nca_min\", \"ncv_max_x\", \"ncv_max_y\", \"ncv_min_x\", \"ncv_min_y\", \"ncv_max\", \"ncv_min\", \"ncv_nca_max_x\", \"ncv_nca_max_y\", \"ncv_nca_min_x\", \"ncv_nca_min_y\", \"ncv_nca_max\", \"ncv_nca_min\", \"ncp_max\", \"ncp_min\", \"std_pre\", 'std_grip', 'mean_grip', \"duration\", 'entropy_S', 'pressure_iqr', 'grip_iqr', 'mv', 'width', 'height', 'displacement', 'energy_x', 'energy_y', 'label']\n",
    "# final.columns = ['ID', 'std_grip', 'entropy_S', 'label']\n",
    "final['ID'] = final['ID'].apply(lambda x: x.split('000')[1]).astype('int32')\n",
    "\n",
    "df = pd.read_excel('PaHaW_csv/corpus_PaHaW.xlsx')\n",
    "df['result'] = np.int32(df['PD status']=='ON')\n",
    "final_data = pd.merge(final, df[['result', 'ID']], on='ID', how='left')\n",
    "### 11, 12, 21, 35, 37, 38, 42, 45-47, 50, 56, 58-59, 63-65, 68, 79, 81, 86, 88, 93 missing\n",
    "# final_data = normalise_data(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = normalise_data(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_HW = pd.DataFrame(final_HW)\n",
    "final_HW.columns = ['ID', \"nvv_x\", \"nvv_y\", \"nvv\", \"npv\", \"jerk_x\", \"jerk_y\", \"jerk\", \"nca_max_x\", \"nca_max_y\", \"nca_min_x\", \"nca_min_y\", \"nca_max\", \"nca_min\", \"ncv_max_x\", \"ncv_max_y\", \"ncv_min_x\", \"ncv_min_y\", \"ncv_max\", \"ncv_min\", \"ncv_nca_max_x\", \"ncv_nca_max_y\", \"ncv_nca_min_x\", \"ncv_nca_min_y\", \"ncv_nca_max\", \"ncv_nca_min\", \"ncp_max\", \"ncp_min\", \"std_pre\", 'std_grip', 'mean_grip', \"duration\", 'entropy_S', 'pressure_iqr', 'grip_iqr', 'mv', 'width', 'height', 'displacement', 'energy_x', 'energy_y', 'label', 'result']\n",
    "# final_HW.columns = ['ID','std_grip', 'entropy_S', 'label', 'result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_HW = normalise_data(final_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.append(final_HW)\n",
    "final_data = normalise_data(final_data)\n",
    "\n",
    "### 67 PaHaW, 76 ParkinsonHW\n",
    "### 51 health controls (15 in parkinson_HW, 36 in PaHaW)\n",
    "### 92 PD patients (61 in parkinson_HW, 31 in PaHaW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(final_HW[[\"nvv_x\", \"nvv_y\", \"nvv\", \"npv\", \"jerk_x\", \"jerk_y\", \"jerk\", \"nca_max_x\", \"nca_max_y\", \"nca_min_x\", \"nca_min_y\", \"nca_max\", \"nca_min\", \"ncv_max_x\", \"ncv_max_y\", \"ncv_min_x\", \"ncv_min_y\", \"ncv_max\", \"ncv_min\", \"ncv_nca_max_x\", \"ncv_nca_max_y\", \"ncv_nca_min_x\", \"ncv_nca_min_y\", \"ncv_nca_max\", \"ncv_nca_min\", \"ncp_max\", \"ncp_min\", \"std_pre\", 'std_grip', 'mean_grip', \"duration\", 'entropy_S', 'pressure_iqr', 'grip_iqr', 'mv', 'width', 'height', 'displacement', 'energy_x', 'energy_y', 'label']])\n",
    "# X = np.array(final_data[['std_grip', 'entropy_S']])\n",
    "y = np.array(final_HW['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40476190476190477\n",
      "0.37\n",
      "0.4666666666666666\n"
     ]
    }
   ],
   "source": [
    "##Decision Tree 10-FOLD\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "acc = []\n",
    "pre = []\n",
    "rec = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    pre.append(precision_score(y_test, pred))\n",
    "    rec.append(recall_score(y_test, pred))\n",
    "\n",
    "print(np.mean(acc))\n",
    "print(np.mean(pre))\n",
    "print(np.mean(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6019047619047619\n",
      "0.642006327006327\n",
      "0.8035714285714285\n"
     ]
    }
   ],
   "source": [
    "##Random Forest 10-FOLD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "acc = []\n",
    "pre = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    pre.append(precision_score(y_test, pred))\n",
    "    rec.append(recall_score(y_test, pred))\n",
    "\n",
    "print(np.mean(acc))\n",
    "print(np.mean(pre))\n",
    "print(np.mean(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5585714285714286\n",
      "0.6883642746142746\n",
      "0.6958573833573833\n"
     ]
    }
   ],
   "source": [
    "##Naive Bayes 10-FOLD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "acc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    pre.append(precision_score(y_test, pred))\n",
    "    rec.append(recall_score(y_test, pred))\n",
    "\n",
    "print(np.mean(acc))\n",
    "print(np.mean(pre))\n",
    "print(np.mean(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100000000000001\n",
      "0.6682738095238094\n",
      "0.7930429292929293\n"
     ]
    }
   ],
   "source": [
    "##LR 10-FOLD\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "acc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    pred = logisticRegr.predict(X_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    pre.append(precision_score(y_test, pred))\n",
    "    rec.append(recall_score(y_test, pred))\n",
    "\n",
    "print(np.mean(acc))\n",
    "print(np.mean(pre))\n",
    "print(np.mean(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##SVM 10-FOLD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "\n",
    "#y = final_data['result']\n",
    "#X = final_data.drop(['ID', 'result'], axis=1)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "#print(kf)\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "predict=[]\n",
    "test=[]\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "\n",
    "    clf = SVC(kernel = 'poly',  gamma=0.7, C=1.0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    print('True:', y_test)\n",
    "    print('False:', pred)\n",
    "    \n",
    "    predict.append(pred)\n",
    "    test.append(X_test)\n",
    "    \n",
    "print(np.mean(acc))\n",
    "print(np.mean(pre))\n",
    "print(np.mean(rec))\n",
    "\n",
    "\n",
    "print(metrics.confusion_matrix(test, predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).to_csv('HW_X_nor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).to_csv('HW_y_nor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
